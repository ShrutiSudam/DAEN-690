{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required libraries\n",
    "# pip install dataframe_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the librarieimport boto3 \n",
    "import botocore \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import dataframe_image as dfi\n",
    "import boto3\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing 30 rows and unlimited columns\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the bucket\n",
    "bucket='daen690-meraki-data'\n",
    "dashboard_bucket = 'daen690-meraki-dashboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the data file name and location for receiving dataset\n",
    "data_receiving = 'gmu_training_receiving_2.xlsx'\n",
    "data_path_receiving = 's3://{}/{}'.format(bucket, data_receiving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the data file name and location for locations dataset\n",
    "data_locations = 'gmu_training_locations.xlsx'\n",
    "data_path_locations = 's3://{}/{}'.format(bucket, data_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the data file name and location for disposals dataset\n",
    "data_disposals = 'gmu_training_disposals.xlsx'\n",
    "data_path_disposals = 's3://{}/{}'.format(bucket, data_disposals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receiving Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket= 'daen690-meraki-data'\n",
    "data_key = 'gmu_training_receiving_2.xlsx'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "df_receiving = pd.read_excel(data_location)\n",
    "df_receiving'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receiving = pd.read_excel(data_path_receiving)\n",
    "df_receiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data dimensions\n",
    "df_receiving.shape\n",
    "\n",
    "# Dataset has 53242 rows Ã— 13 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking columns, data type and missing values\n",
    "df_receiving.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receiving.isna().sum()\n",
    "\n",
    "# There are 49,757 missing values in the column 'MISCLASSIFIED_FAP', rest all columns don't have any missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicated rows\n",
    "df_receiving[df_receiving.duplicated()]\n",
    "\n",
    "# there are no duplicated rows in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the column names\n",
    "df_receiving.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming some of the column names\n",
    "df_receiving.columns = ['TRANS_NUMBER', 'BUSINESS_UNIT', 'CREATION_METHOD', 'OPR_ID', \n",
    "       'ASSET_SUBTYPE', 'ASSET_CLASS', 'ASSET_CLASS_DESCR',\n",
    "       'ACQUISITION_DT', 'AGENCY_CODE', 'COST', 'LOCATION_TYPE',\n",
    "       'FAP_LIKE_FLAG', 'MISCLASSIFIED_FAP']\n",
    "\n",
    "df_receiving.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the data type of 'TRANS_NUMBER' and 'ASSET_CLASS' to string \n",
    "df_receiving['TRANS_NUMBER'] = df_receiving['TRANS_NUMBER'].astype('str')\n",
    "df_receiving['ASSET_CLASS'] = df_receiving['ASSET_CLASS'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for data types of all columns again\n",
    "df_receiving.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking details for numerical columns\n",
    "df_receiving.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Asset Cost\n",
    "sns.distplot(df_receiving.COST, kde=True).set_title('Histogram of Asset Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking details for categorical columns\n",
    "df_receiving.describe(include=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"CREATION_METHOD\" column\n",
    "df_receiving.groupby(\"CREATION_METHOD\").agg(\n",
    "    TRANSACTION_COUNT = (\"CREATION_METHOD\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receiving.CREATION_METHOD = df_receiving.CREATION_METHOD.replace(\n",
    "    ['AM Page', 'PI Add'], ['Manual Creation', 'Manual Creation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"CREATION_METHOD\" column again\n",
    "df_receiving.groupby(\"CREATION_METHOD\").agg(\n",
    "    TRANSACTION_COUNT = (\"CREATION_METHOD\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation Method-wise Transaction Count\n",
    "sns.countplot(x=\"CREATION_METHOD\", data=df_receiving).set_title('Creation Method-wise Transaction Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"ASSET_SUBTYPE\" column\n",
    "ast_typ_count = df_receiving.groupby(\"ASSET_SUBTYPE\").agg(\n",
    "                    TRANSACTION_COUNT = (\"ASSET_SUBTYPE\", \"count\"),\n",
    "                ).sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()\n",
    "ast_typ_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asset Subtype-wise Transaction Count\n",
    "p = sns.barplot(x='ASSET_SUBTYPE', y='TRANSACTION_COUNT', data=ast_typ_count)\n",
    "p.set_xticklabels(p.get_xticklabels(),rotation=90)\n",
    "p.set_title('Asset Subtype-wise Transaction Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"LOCATION_TYPE\" column\n",
    "loc_typ_count = df_receiving.groupby(\"LOCATION_TYPE\").agg(\n",
    "    TRANSACTION_COUNT = (\"LOCATION_TYPE\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()\n",
    "loc_typ_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location type-wise Transaction Count\n",
    "p = sns.barplot(x='LOCATION_TYPE', y='TRANSACTION_COUNT', data=loc_typ_count)\n",
    "p.set_xticklabels(p.get_xticklabels(),rotation=90)\n",
    "p.set_title('Location type-wise Transaction Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"FAP_LIKE_FLAG\" column\n",
    "fap_flag_count = df_receiving.groupby(\"FAP_LIKE_FLAG\").agg(\n",
    "                    TRANSACTION_COUNT = (\"FAP_LIKE_FLAG\", \"count\"),\n",
    "                ).sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()\n",
    "fap_flag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAP Like Flag-wise Transaction Count\n",
    "p = sns.barplot(x='FAP_LIKE_FLAG', y='TRANSACTION_COUNT', data=fap_flag_count)\n",
    "p.set_title('FAP Like Flag-wise Transaction Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"MISCLASSIFIED_FAP\" column\n",
    "df_receiving.groupby(\"MISCLASSIFIED_FAP\").agg(\n",
    "    TRANSACTION_COUNT = (\"MISCLASSIFIED_FAP\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receiving.MISCLASSIFIED_FAP = df_receiving.MISCLASSIFIED_FAP.fillna('Correct_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"MISCLASSIFIED_FAP\" column again\n",
    "miscl_fap_count = df_receiving.groupby(\"MISCLASSIFIED_FAP\").agg(\n",
    "    TRANSACTION_COUNT = (\"MISCLASSIFIED_FAP\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()\n",
    "miscl_fap_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misclassified FAP-wise Transaction Count\n",
    "p = sns.barplot(x='MISCLASSIFIED_FAP', y='TRANSACTION_COUNT', data=miscl_fap_count)\n",
    "p.set_xticklabels(p.get_xticklabels(),rotation=70)\n",
    "p.set_title('Misclassified FAP-wise Transaction Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column 'CORRECT_CLASSIFICATION'\n",
    "mis_list = ['Should be FAP based on Asset Class', 'Should not be FAP based on Asset Class', 'FAP not in Approved Location']\n",
    "list_cor_class = []\n",
    "\n",
    "for idx in df_receiving.index:\n",
    "    cor = df_receiving['MISCLASSIFIED_FAP'][idx]\n",
    "    if cor in mis_list:\n",
    "        list_cor_class.append(\"No\")\n",
    "    elif cor == 'Correct_classification':\n",
    "        list_cor_class.append(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_cor_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receiving.insert(13, 'CORRECT_CLASSIFICATION', list_cor_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receiving.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check values in MISCLASSIFIED_FAP for CORRECT_CLASSIFICATION = No \n",
    "df_receiving[df_receiving['CORRECT_CLASSIFICATION'] == 'No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receiving.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at top users based on number of transactions\n",
    "df_receiving.groupby(\"OPR_ID\").agg(\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing 100 rows \n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering incorrect classifications and finding top users based on number of transactions\n",
    "user_trans = df_receiving[df_receiving['CORRECT_CLASSIFICATION'].str.contains('No')].groupby(\"OPR_ID\").agg(\n",
    "            TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "            ).sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index().round(2)\n",
    "user_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for top users with maximum erroneous transactions\n",
    "sns.barplot(x=\"OPR_ID\", y=\"TRANSACTION_COUNT\", data=user_trans)\n",
    "plt.title(\"Top users with maximum erroneous transactions\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches( 16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at top users based on total asset cost that they are handling\n",
    "df_receiving.groupby(\"OPR_ID\").agg(\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    ").sort_values(by = \"TOTAL_ASSET_COST\", ascending= False).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Filtering incorrect classifications and finding top users based on total asset cost that they are handling\n",
    "user_tot_cost = df_receiving[df_receiving['CORRECT_CLASSIFICATION'].str.contains('No')].groupby(\"OPR_ID\").agg(\n",
    "                TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    "                ).sort_values(by = \"TOTAL_ASSET_COST\", ascending= False).reset_index().round(2)\n",
    "user_tot_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for top erring users based on total assets cost\n",
    "sns.barplot(x=\"OPR_ID\", y=\"TOTAL_ASSET_COST\", data=user_tot_cost)\n",
    "plt.title(\"Top erring users based on total assets cost\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying top users based on total asset cost, count and average cost \n",
    "df_receiving.groupby(\"OPR_ID\").agg(\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "    AVG_ASSET_COST = (\"COST\", \"mean\")\n",
    ").sort_values(by = \"AVG_ASSET_COST\", ascending= False).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering incorrect classifications and identifying top users based on total asset cost, count and average cost \n",
    "user_avg_cost = df_receiving[df_receiving['CORRECT_CLASSIFICATION'].str.contains('No')].groupby(\"OPR_ID\").agg(\n",
    "                TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    "                TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "                AVG_ASSET_COST = (\"COST\", \"mean\")\n",
    "                ).sort_values(by = \"AVG_ASSET_COST\", ascending= False).reset_index().round(2)\n",
    "user_avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for top erring users based on average assets cost\n",
    "sns.barplot(x=\"OPR_ID\", y=\"AVG_ASSET_COST\", data=user_avg_cost)\n",
    "plt.title(\"Top erring users based on average asset cost\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering incorrect classifications and identifying top BUSINESS_UNIT based on total asset cost, count and average cost \n",
    "bu_rec_totcost = df_receiving[df_receiving['CORRECT_CLASSIFICATION'].str.contains('No')].groupby(\"BUSINESS_UNIT\").agg(\n",
    "                TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    "                TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "                AVG_ASSET_COST = (\"COST\", \"mean\")\n",
    "                ).sort_values(by = \"TOTAL_ASSET_COST\", ascending= False).reset_index().round(2)\n",
    "bu_rec_totcost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering incorrect classifications and identifying top AGENCY_CODE based on total asset cost, count and average cost \n",
    "agc_rec_totcost = df_receiving[df_receiving['CORRECT_CLASSIFICATION'].str.contains('No')].groupby(\"AGENCY_CODE\").agg(\n",
    "                TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    "                TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "                AVG_ASSET_COST = (\"COST\", \"mean\")\n",
    "                ).sort_values(by = \"TOTAL_ASSET_COST\", ascending= False).reset_index().round(2)\n",
    "agc_rec_totcost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Business_Unit-wise top users based on total asset cost\n",
    "df_receiving.groupby([\"BUSINESS_UNIT\",\"OPR_ID\"]).agg(\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\")\n",
    ").sort_values(by = [\"BUSINESS_UNIT\", \"TOTAL_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering incorrect classifications and identifying Business_Unit-wise top users based on total asset cost\n",
    "bu_user = df_receiving[df_receiving['CORRECT_CLASSIFICATION'].str.contains('No')].groupby([\"BUSINESS_UNIT\",\"OPR_ID\"]).agg(\n",
    "            TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "            TOTAL_ASSET_COST = (\"COST\", \"sum\")\n",
    "            ).sort_values(by = [\"BUSINESS_UNIT\", \"TOTAL_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)\n",
    "bu_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying AGENCY_CODE-wise top users based on total asset cost\n",
    "df_receiving.groupby([\"AGENCY_CODE\",\"OPR_ID\"]).agg(\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\")\n",
    ").sort_values(by = [\"AGENCY_CODE\", \"TOTAL_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering incorrect classifications and identifying AGENCY_CODE-wise top users based on total asset cost\n",
    "df_receiving[df_receiving['CORRECT_CLASSIFICATION'].str.contains('No')].groupby([\"AGENCY_CODE\",\"OPR_ID\"]).agg(\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\")\n",
    ").sort_values(by = [\"AGENCY_CODE\", \"TOTAL_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe rec_df by subsetting key attributes influencing error rate\n",
    "rec_df = df_receiving[['BUSINESS_UNIT','CREATION_METHOD', 'OPR_ID', 'AGENCY_CODE', 'COST', 'CORRECT_CLASSIFICATION']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.describe(include = np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.CREATION_METHOD.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the levels within CREATION_METHOD to '0' and '1'\n",
    "rec_df.CREATION_METHOD = rec_df.CREATION_METHOD.replace(['Purch. Req', 'Manual Creation'], ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.CREATION_METHOD.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.CORRECT_CLASSIFICATION.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the levels within CORRECT_CLASSIFICATION to '0' and '1'\n",
    "rec_df.CORRECT_CLASSIFICATION = rec_df.CORRECT_CLASSIFICATION.replace(['Yes', 'No'], ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.CORRECT_CLASSIFICATION.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns for differentiation\n",
    "rec_df.columns = ['REC_LOCATION', 'REC_CREATION', 'REC_USER', 'REC_AGENCY', 'REC_COST', 'REC_CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.groupby(['REC_USER','REC_LOCATION', 'REC_AGENCY', 'REC_CREATION','REC_CLASS']).agg(\n",
    "    TRANS_COUNT = (\"REC_COST\", \"count\"),\n",
    "    ASSET_COST = (\"REC_COST\", \"sum\")\n",
    ").sort_values(by = [\"REC_USER\", \"ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df[(rec_df.REC_CREATION =='1') | (rec_df.REC_CLASS =='1')].groupby(['REC_USER','REC_LOCATION', 'REC_AGENCY', 'REC_CREATION','REC_CLASS']).agg(\n",
    "    TRANS_COUNT = (\"REC_COST\", \"count\"),\n",
    "    ASSET_COST = (\"REC_COST\", \"sum\")\n",
    ").sort_values(by = [\"REC_USER\", \"ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with REC_CREATION =='1' and obtaining user-wise count and total asset cost\n",
    "rec_creation = rec_df[rec_df.REC_CREATION =='1'].groupby(['REC_USER', 'REC_CREATION']).agg(\n",
    "                    REC_TRANS_COUNT = (\"REC_COST\", \"count\"),\n",
    "                    REC_ASSET_COST = (\"REC_COST\", \"sum\")\n",
    "                ).sort_values(by = [\"REC_USER\", \"REC_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)\n",
    "rec_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with REC_CLASS =='1' and obtaining user-wise count and total asset cost\n",
    "rec_class = rec_df[rec_df.REC_CLASS =='1'].groupby(['REC_USER','REC_CLASS']).agg(\n",
    "                REC_TRANS_COUNT = (\"REC_COST\", \"count\"),\n",
    "                REC_ASSET_COST = (\"REC_COST\", \"sum\")\n",
    "            ).sort_values(by = [\"REC_USER\", \"REC_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)\n",
    "rec_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creating a dataframe with REC_CREATION =='1' and obtaining location-wise user count\n",
    "loc_1 = rec_df[rec_df.REC_CREATION =='1'].groupby(['REC_LOCATION','REC_USER']).agg(\n",
    "            COUNT = (\"REC_USER\", \"count\") \n",
    "        ).sort_values(by = ['REC_LOCATION', 'COUNT'], ascending= [True, False] ).reset_index()\n",
    "loc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creating a dataframe with REC_CLASS =='1' and obtaining location-wise user count\n",
    "loc_2 = rec_df[rec_df.REC_CLASS =='1'].groupby(['REC_LOCATION','REC_USER']).agg(\n",
    "            COUNT = (\"REC_USER\", \"count\") \n",
    "        ).sort_values(by = ['REC_LOCATION', 'COUNT'], ascending= [True, False] ).reset_index()\n",
    "loc_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations = pd.read_excel(data_path_locations)\n",
    "df_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicated rows\n",
    "df_locations[df_locations.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking column names\n",
    "df_locations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column names\n",
    "df_locations.columns = ['TRANS_NUMBER', 'BUSINESS_UNIT', 'AGENCY_CODE', 'ASSET_SUBTYPE',\n",
    "       'LOCATION_DATE', 'DOC_NUM', 'LOCATION_TYPE', 'FORM_CHECK',\n",
    "       'ENTERED_BY', 'COST', 'LOCATION', 'VALID_DOC_FLAG']\n",
    "\n",
    "df_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the data type of 'TRANS_NUMBER' and 'VALID_DOC_FLAG' to string\n",
    "df_locations.TRANS_NUMBER = df_locations.TRANS_NUMBER.astype('str')\n",
    "df_locations.VALID_DOC_FLAG = df_locations.VALID_DOC_FLAG.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'COST' to data type float\n",
    "# df_locations.COST = df_locations.COST.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.VALID_DOC_FLAG = df_locations.VALID_DOC_FLAG.replace(['1', '0'], ['Yes', 'No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'COST' to data type float\n",
    "# df_locations.COST = df_locations.COST.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations[df_locations['COST'].str.contains(\"'-\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.loc[53738, 'COST'] = 445.9\n",
    "df_locations.loc[81087, 'COST'] = 334.45\n",
    "df_locations.loc[81786, 'COST'] = 114.64\n",
    "df_locations.loc[[97261,97480], 'COST'] = 135.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.COST = df_locations.COST.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations = df_locations.dropna(subset=['ENTERED_BY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.describe(include=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"ASSET_SUBTYPE\" column\n",
    "df_locations.groupby(\"ASSET_SUBTYPE\").agg(\n",
    "    TRANSACTION_COUNT = (\"ASSET_SUBTYPE\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"VALID_DOC_FLAG\" column\n",
    "df_locations.groupby(\"VALID_DOC_FLAG\").agg(\n",
    "    TRANSACTION_COUNT = (\"VALID_DOC_FLAG\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at top users based on number of transactions\n",
    "df_locations.groupby(\"ENTERED_BY\").agg(\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering incorrect classifications and finding top users based on number of transactions\n",
    "user_loc_trans = df_locations[df_locations['VALID_DOC_FLAG'].str.contains('No')].groupby(\"ENTERED_BY\").agg(\n",
    "            TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "            ).sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index().round(2)\n",
    "user_loc_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for top 50 users with maximum erroneous transactions\n",
    "sns.barplot(x=\"ENTERED_BY\", y=\"TRANSACTION_COUNT\", data=user_loc_trans[:50])\n",
    "plt.title(\"Top users with maximum erroneous transactions\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at top users based on total asset cost that they are handling\n",
    "df_locations.groupby(\"ENTERED_BY\").agg(\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    ").sort_values(by = \"TOTAL_ASSET_COST\", ascending= False).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Filtering incorrect classifications and finding top users based on total asset cost that they are handling\n",
    "user_loc_totcost = df_locations[df_locations['VALID_DOC_FLAG'].str.contains('No')].groupby(\"ENTERED_BY\").agg(\n",
    "                TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    "                ).sort_values(by = \"TOTAL_ASSET_COST\", ascending= False).reset_index().round(2)\n",
    "user_loc_totcost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for top erring users based on total assets cost\n",
    "sns.barplot(x=\"ENTERED_BY\", y=\"TOTAL_ASSET_COST\", data=user_loc_totcost[:50])\n",
    "plt.title(\"Top erring users based on total assets cost\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying top users based on total asset cost, count and average cost \n",
    "df_locations.groupby(\"ENTERED_BY\").agg(\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "    AVG_ASSET_COST = (\"COST\", \"mean\")\n",
    ").sort_values(by = \"AVG_ASSET_COST\", ascending= False).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering incorrect classifications and identifying top users based on total asset cost, count and average cost \n",
    "user_loc_avgcost = df_locations[df_locations['VALID_DOC_FLAG'].str.contains('No')].groupby(\"ENTERED_BY\").agg(\n",
    "                TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    "                TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "                AVG_ASSET_COST = (\"COST\", \"mean\")\n",
    "                ).sort_values(by = \"AVG_ASSET_COST\", ascending= False).reset_index().round(2)\n",
    "user_loc_avgcost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for top erring users based on average assets cost\n",
    "sns.barplot(x=\"ENTERED_BY\", y=\"AVG_ASSET_COST\", data=user_loc_avgcost[:50])\n",
    "plt.title(\"Top erring users based on average asset cost\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Business_Unit-wise top users based on total asset cost\n",
    "df_locations.groupby([\"BUSINESS_UNIT\",\"ENTERED_BY\"]).agg(\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\")\n",
    ").sort_values(by = [\"BUSINESS_UNIT\", \"TOTAL_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering incorrect classifications and identifying Business_Unit-wise top users based on total asset cost\n",
    "bu_loc_user = df_locations[df_locations['VALID_DOC_FLAG'].str.contains('No')].groupby([\"BUSINESS_UNIT\",\"ENTERED_BY\"]).agg(\n",
    "            TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "            TOTAL_ASSET_COST = (\"COST\", \"sum\")\n",
    "            ).sort_values(by = [\"BUSINESS_UNIT\", \"TOTAL_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)\n",
    "bu_loc_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying AGENCY_CODE-wise top users based on total asset cost\n",
    "df_locations.groupby([\"AGENCY_CODE\",\"ENTERED_BY\"]).agg(\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\")\n",
    ").sort_values(by = [\"AGENCY_CODE\", \"TOTAL_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering incorrect classifications and identifying AGENCY_CODE-wise top users based on total asset cost\n",
    "df_locations[df_locations['VALID_DOC_FLAG'].str.contains('No')].groupby([\"AGENCY_CODE\",\"ENTERED_BY\"]).agg(\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\")\n",
    ").sort_values(by = [\"AGENCY_CODE\", \"TOTAL_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe loc_df by subsetting key attributes influencing error rate\n",
    "loc_df = df_locations[['BUSINESS_UNIT','AGENCY_CODE', 'ENTERED_BY', 'COST', 'VALID_DOC_FLAG']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df.VALID_DOC_FLAG.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the levels for VALID_DOC_FLAG to '0' and '1'\n",
    "loc_df.VALID_DOC_FLAG = loc_df.VALID_DOC_FLAG.replace(['Yes', 'No'], ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df.VALID_DOC_FLAG.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns for differentiation\n",
    "loc_df.columns = ['LOC_LOCATION', 'LOC_AGENCY', 'LOC_USER', 'LOC_COST', 'LOC_DOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df.groupby(['LOC_USER','LOC_LOCATION', 'LOC_AGENCY', 'LOC_DOC']).agg(\n",
    "    LOC_TRANS_COUNT = (\"LOC_COST\", \"count\"),\n",
    "    LOC_ASSET_COST = (\"LOC_COST\", \"sum\")\n",
    ").sort_values(by = [\"LOC_USER\", \"LOC_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with LOC_DOC =='1' and user-wise transaction count and total asset cost \n",
    "loc_doc = loc_df[loc_df.LOC_DOC =='1'].groupby(\n",
    "                ['LOC_USER', 'LOC_DOC']).agg(\n",
    "                LOC_TRANS_COUNT = (\"LOC_COST\", \"count\"),\n",
    "                LOC_ASSET_COST = (\"LOC_COST\", \"sum\")\n",
    "            ).sort_values(by = [\"LOC_USER\", \"LOC_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)\n",
    "loc_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for LOC_DOC =='1' and location-wise user list\n",
    "loc_3 = loc_df[loc_df.LOC_DOC =='1'].groupby(['LOC_LOCATION','LOC_USER']).agg(\n",
    "            COUNT = (\"LOC_USER\", \"count\") \n",
    "        ).sort_values(by = ['LOC_LOCATION', 'COUNT'], ascending= [True, False]).reset_index()\n",
    "loc_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disposals Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals = pd.read_excel(data_path_disposals)\n",
    "df_disposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicated rows\n",
    "df_disposals[df_disposals.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the column names\n",
    "df_disposals.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the column names\n",
    "df_disposals.columns = ['TRANS_NUMBER', 'BUSINESS_UNIT', 'ASSET_DESCR', 'ASSET_SUBTYPE', 'DISP_DOC',\n",
    "       'EXPECTED_RET_DATE', 'ACTUAL_RET_DATE', 'SCAN_TYPE', 'COST', 'USER_ID']\n",
    "df_disposals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the data type of 'TRANS_NUMBER' to string and 'COST' to 'float' \n",
    "df_disposals['TRANS_NUMBER'] = df_disposals['TRANS_NUMBER'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_disposals['COST'] = df_disposals['COST'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals[df_disposals['COST'].str.contains(\"'-\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.loc[25853, 'COST'] = 61.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals['COST'] = df_disposals['COST'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for data types of all columns again\n",
    "df_disposals.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking details for numerical columns\n",
    "df_disposals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking details for categorical columns\n",
    "df_disposals.describe(include=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"ASSET_SUBTYPE\" column\n",
    "df_disposals.groupby(\"ASSET_SUBTYPE\").agg(\n",
    "    TRANSACTION_COUNT = (\"ASSET_SUBTYPE\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"DISP_DOC\" column\n",
    "df_disposals.groupby(\"DISP_DOC\").agg(\n",
    "    TRANSACTION_COUNT = (\"DISP_DOC\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the 'na' values in \"SCAN_TYPE\" column as 'Missing'\n",
    "df_disposals.SCAN_TYPE = df_disposals.SCAN_TYPE.fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"SCAN_TYPE\" column\n",
    "df_disposals.groupby(\"SCAN_TYPE\").agg(\n",
    "    TRANSACTION_COUNT = (\"SCAN_TYPE\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.SCAN_TYPE = df_disposals.SCAN_TYPE.replace(\n",
    "    ['Scan Transfer', 'Discovered', 'Invalid'], ['Scanned', 'Scanned', 'Manual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"SCAN_TYPE\" column again\n",
    "df_disposals.groupby(\"SCAN_TYPE\").agg(\n",
    "    TRANSACTION_COUNT = (\"SCAN_TYPE\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'EXPECTED_RET_YEAR'\n",
    "list_exp_retyear = []\n",
    "for idx in df_disposals.index:\n",
    "    list_exp_retyear.append(df_disposals['EXPECTED_RET_DATE'][idx].split(\".\")[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_exp_retyear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.insert(6, 'EXPECTED_RET_YEAR', list_exp_retyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'ACTUAL_RET_YEAR'\n",
    "list_act_retyear = []\n",
    "for idx in df_disposals.index:\n",
    "    list_act_retyear.append(df_disposals['ACTUAL_RET_DATE'][idx].split(\".\")[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_act_retyear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.insert(8, 'ACTUAL_RET_YEAR', list_act_retyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the EXPECTED_RET_YEAR and ACTUAL_RET_YEAR to check the difference between them\n",
    "df_disposals['EXPECTED_RET_YEAR'] = df_disposals['EXPECTED_RET_YEAR'].astype('int')\n",
    "df_disposals['ACTUAL_RET_YEAR'] = df_disposals['ACTUAL_RET_YEAR'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'CORRECT_RET_YEAR'\n",
    "list_ret_year = []\n",
    "for idx in df_disposals.index:\n",
    "    if(df_disposals['ACTUAL_RET_YEAR'][idx] - df_disposals['EXPECTED_RET_YEAR'][idx]) >=0:\n",
    "        list_ret_year.append('Yes')\n",
    "    else:\n",
    "        list_ret_year.append('No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_ret_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.insert(9, 'CORRECT_RET_YEAR', list_ret_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values and count for \"CORRECT_RET_YEAR\" column again\n",
    "df_disposals.groupby(\"CORRECT_RET_YEAR\").agg(\n",
    "    TRANSACTION_COUNT = (\"CORRECT_RET_YEAR\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at top users based on number of transactions\n",
    "df_disposals.groupby(\"USER_ID\").agg(\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    ").sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for DISP_DOC = No and finding top users based on number of transactions\n",
    "user_disp_trans1 = df_disposals[df_disposals['DISP_DOC'].str.contains('No')].groupby(\"USER_ID\").agg(\n",
    "            TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "            ).sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index().round(2)\n",
    "user_disp_trans1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for top users with maximum erroneous transactions\n",
    "sns.barplot(x=\"USER_ID\", y=\"TRANSACTION_COUNT\", data=user_disp_trans1)\n",
    "plt.title(\"Top users with maximum transactions with error in Scan Type\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for SCAN_TYPE = Missing, Manual and finding top users based on number of transactions\n",
    "user_disp_trans2 = df_disposals[df_disposals.SCAN_TYPE.isin(['Missing', 'Manual'])].groupby(\"USER_ID\").agg(\n",
    "            TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "            ).sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index().round(2)\n",
    "user_disp_trans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for top users with maximum erroneous transactions\n",
    "sns.barplot(x=\"USER_ID\", y=\"TRANSACTION_COUNT\", data=user_disp_trans2[:50])\n",
    "plt.title(\"Top users with maximum transactions with error in Scan Type\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for CORRECT_RET_YEAR = No and finding top users based on number of transactions\n",
    "user_disp_trans3 = df_disposals[df_disposals['CORRECT_RET_YEAR'].str.contains('No')].groupby(\"USER_ID\").agg(\n",
    "            TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "            ).sort_values(by = \"TRANSACTION_COUNT\", ascending= False).reset_index().round(2)\n",
    "user_disp_trans3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for top users with maximum erroneous transactions\n",
    "sns.barplot(x=\"USER_ID\", y=\"TRANSACTION_COUNT\", data=user_disp_trans3[:50])\n",
    "plt.title(\"Top users with maximum transactions with error in Retirement Years\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at top users based on total asset cost that they are handling\n",
    "df_disposals.groupby(\"USER_ID\").agg(\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    ").sort_values(by = \"TOTAL_ASSET_COST\", ascending= False).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Filtering DISP_DOC = No and finding top users based on total asset cost that they are handling\n",
    "user_disp_totcost1 = df_disposals[df_disposals['DISP_DOC'].str.contains('No')].groupby(\"USER_ID\").agg(\n",
    "                TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    "                ).sort_values(by = \"TOTAL_ASSET_COST\", ascending= False).reset_index().round(2)\n",
    "user_disp_totcost1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for top erring users based on total assets cost\n",
    "sns.barplot(x=\"USER_ID\", y=\"TOTAL_ASSET_COST\", data=user_disp_totcost1)\n",
    "plt.title(\"Top erring users based on total assets cost\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying top users based on total asset cost, count and average cost \n",
    "df_disposals.groupby(\"USER_ID\").agg(\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "    AVG_ASSET_COST = (\"COST\", \"mean\")\n",
    ").sort_values(by = \"AVG_ASSET_COST\", ascending= False).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering DISP_DOC = No and identifying top users based on total asset cost, count and average cost \n",
    "user_disp_avgcost1 = df_disposals[df_disposals['DISP_DOC'].str.contains('No')].groupby(\"USER_ID\").agg(\n",
    "                TOTAL_ASSET_COST = (\"COST\", \"sum\"),\n",
    "                TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "                AVG_ASSET_COST = (\"COST\", \"mean\")\n",
    "                ).sort_values(by = \"AVG_ASSET_COST\", ascending= False).reset_index().round(2)\n",
    "user_disp_avgcost1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for top erring users based on average assets cost\n",
    "sns.barplot(x=\"USER_ID\", y=\"AVG_ASSET_COST\", data=user_disp_avgcost1)\n",
    "plt.title(\"Top erring users based on average asset cost\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Business_Unit-wise top users based on total asset cost\n",
    "df_disposals.groupby([\"BUSINESS_UNIT\",\"USER_ID\"]).agg(\n",
    "    TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "    TOTAL_ASSET_COST = (\"COST\", \"sum\")\n",
    ").sort_values(by = [\"BUSINESS_UNIT\", \"TOTAL_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering DISP_DOC = No and identifying Business_Unit-wise top users based on total asset cost\n",
    "bu_disp_user1 = df_disposals[df_disposals['DISP_DOC'].str.contains('No')].groupby([\"BUSINESS_UNIT\",\"USER_ID\"]).agg(\n",
    "            TRANSACTION_COUNT = (\"COST\", \"count\"),\n",
    "            TOTAL_ASSET_COST = (\"COST\", \"sum\")\n",
    "            ).sort_values(by = [\"BUSINESS_UNIT\", \"TOTAL_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)\n",
    "bu_disp_user1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disposals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe disp_df by subsetting key attributes influencing error rate\n",
    "disp_df = df_disposals[['BUSINESS_UNIT','DISP_DOC', 'CORRECT_RET_YEAR', 'SCAN_TYPE', 'COST', 'USER_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.DISP_DOC.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the levels for DISP_DOC to '0' and '1'\n",
    "disp_df.DISP_DOC = disp_df.DISP_DOC.replace(['Yes', 'No'], ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.DISP_DOC.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.CORRECT_RET_YEAR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the levels for CORRECT_RET_YEAR to '0' and '1'\n",
    "disp_df.CORRECT_RET_YEAR = disp_df.CORRECT_RET_YEAR.replace(['Yes', 'No'], ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.CORRECT_RET_YEAR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the levels for SCAN_TYPE to '0', '0.5' and '1'\n",
    "disp_df.SCAN_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.SCAN_TYPE = disp_df.SCAN_TYPE.replace(['Scanned', 'Manual', 'Missing'], ['0', '0.5', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.SCAN_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns for differentiation\n",
    "disp_df.columns = ['DISP_LOCATION', 'DISP_DOC', 'DISP_RET_YEAR', 'DISP_SCAN_TYPE', 'DISP_COST', 'DISP_USER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.groupby(['DISP_USER','DISP_LOCATION', 'DISP_DOC', 'DISP_RET_YEAR','DISP_SCAN_TYPE']).agg(\n",
    "    DISP_TRANS_COUNT = (\"DISP_COST\", \"count\"),\n",
    "    DISP_ASSET_COST = (\"DISP_COST\", \"sum\")\n",
    ").sort_values(by = [\"DISP_USER\", \"DISP_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df[(disp_df.DISP_DOC =='1') | (disp_df.DISP_RET_YEAR =='1') | (disp_df.DISP_SCAN_TYPE =='1') | (disp_df.DISP_SCAN_TYPE =='0.5')].groupby(\n",
    "    ['DISP_USER','DISP_LOCATION', 'DISP_DOC', 'DISP_RET_YEAR','DISP_SCAN_TYPE']).agg(\n",
    "    DISP_TRANS_COUNT = (\"DISP_COST\", \"count\"),\n",
    "    DISP_ASSET_COST = (\"DISP_COST\", \"sum\")\n",
    ").sort_values(by = [\"DISP_USER\", \"DISP_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for DISP_DOC =='1' and user-wise transaction count and total asset cost\n",
    "disp_doc = disp_df[disp_df.DISP_DOC =='1'].groupby(\n",
    "                ['DISP_USER','DISP_DOC']).agg(\n",
    "                DISP_TRANS_COUNT = (\"DISP_COST\", \"count\"),\n",
    "                DISP_ASSET_COST = (\"DISP_COST\", \"sum\")\n",
    "            ).sort_values(by = [\"DISP_USER\", \"DISP_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)\n",
    "disp_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for DISP_RET_YEAR =='1' and user-wise transaction count and total asset cost\n",
    "disp_retyear = disp_df[disp_df.DISP_RET_YEAR =='1'].groupby(\n",
    "                    ['DISP_USER','DISP_RET_YEAR']).agg(\n",
    "                    DISP_TRANS_COUNT = (\"DISP_COST\", \"count\"),\n",
    "                    DISP_ASSET_COST = (\"DISP_COST\", \"sum\")\n",
    "                ).sort_values(by = [\"DISP_USER\", \"DISP_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)\n",
    "disp_retyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for DISP_SCAN_TYPE =='1' and user-wise transaction count and total asset cost\n",
    "disp_scan_mis = disp_df[(disp_df.DISP_SCAN_TYPE =='1')].groupby(\n",
    "                ['DISP_USER', 'DISP_SCAN_TYPE']).agg(\n",
    "                DISP_TRANS_COUNT = (\"DISP_COST\", \"count\"),\n",
    "                DISP_ASSET_COST = (\"DISP_COST\", \"sum\")\n",
    "            ).sort_values(by = [\"DISP_USER\", \"DISP_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)\n",
    "disp_scan_mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for DISP_SCAN_TYPE =='0.5' and user-wise transaction count and total asset cost\n",
    "disp_scan_man = disp_df[(disp_df.DISP_SCAN_TYPE =='0.5')].groupby(\n",
    "                ['DISP_USER', 'DISP_SCAN_TYPE']).agg(\n",
    "                DISP_TRANS_COUNT = (\"DISP_COST\", \"count\"),\n",
    "                DISP_ASSET_COST = (\"DISP_COST\", \"sum\")\n",
    "            ).sort_values(by = [\"DISP_USER\", \"DISP_ASSET_COST\"], ascending=[True, False]).reset_index().round(2)\n",
    "disp_scan_man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for DISP_DOC =='1' and location-wise user list\n",
    "loc_4 = disp_df[disp_df.DISP_DOC =='1'].groupby(['DISP_LOCATION','DISP_USER']).agg(\n",
    "            COUNT = (\"DISP_USER\", \"count\") \n",
    "        ).sort_values(by = ['DISP_LOCATION', 'COUNT'], ascending= [True, False]).reset_index()\n",
    "loc_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for DISP_RET_YEAR =='1' and location-wise user list\n",
    "loc_5 = disp_df[disp_df.DISP_RET_YEAR =='1'].groupby(['DISP_LOCATION','DISP_USER']).agg(\n",
    "            COUNT = (\"DISP_USER\", \"count\") \n",
    "        ).sort_values(by = ['DISP_LOCATION', 'COUNT'], ascending= [True, False]).reset_index()\n",
    "loc_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for DISP_SCAN_TYPE =='0.5' or '1' and location-wise user list\n",
    "loc_6 = disp_df[(disp_df.DISP_SCAN_TYPE == '0.5') |(disp_df.DISP_SCAN_TYPE == '1')].groupby(['DISP_LOCATION','DISP_USER']).agg(\n",
    "            COUNT = (\"DISP_USER\", \"count\") \n",
    "        ).sort_values(by = ['DISP_LOCATION', 'COUNT'], ascending= [True, False]).reset_index()\n",
    "loc_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Scoring Matrix Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining dataframes rec_creation, rec_class, loc_doc, disp_doc, disp_retyear, disp_scan_mis, disp_scan_man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df1 = pd.merge(rec_creation, rec_class, how ='outer', on ='REC_USER') \n",
    "joined_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df1.columns = ['REC_USER', 'REC_CREATION', 'REC_CREAT_COUNT', 'REC_CREAT_COST', \n",
    "                      'REC_CLASS', 'REC_CLASS_COUNT', 'REC_CLASS_COST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df2 = pd.merge(disp_doc, disp_retyear, how ='outer', on ='DISP_USER') \n",
    "joined_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df2.columns = ['DISP_USER', 'DISP_DOC', 'DISP_DOC_COUNT', 'DISP_DOC_COST', \n",
    "                      'DISP_RET_YEAR', 'DISP_RET_COUNT', 'DISP_RET_COST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df3 = pd.merge(joined_df2, disp_scan_mis, how ='outer', on ='DISP_USER') \n",
    "joined_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df3.columns = ['DISP_USER', 'DISP_DOC', 'DISP_DOC_COUNT', 'DISP_DOC_COST', \n",
    "                      'DISP_RET_YEAR', 'DISP_RET_COUNT', 'DISP_RET_COST', 'DISP_SCAN_MIS', \n",
    "                      'DISP_SCAN_MIS_COUNT', 'DISP_SCAN_MIS_COST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df4 = pd.merge(joined_df3, disp_scan_man, how ='outer', on ='DISP_USER') \n",
    "joined_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df4.columns = ['DISP_USER', 'DISP_DOC', 'DISP_DOC_COUNT', 'DISP_DOC_COST', \n",
    "                      'DISP_RET_YEAR', 'DISP_RET_COUNT', 'DISP_RET_COST', 'DISP_SCAN_MIS', \n",
    "                      'DISP_SCAN_MIS_COUNT', 'DISP_SCAN_MIS_COST', 'DISP_SCAN_MAN', \n",
    "                      'DISP_SCAN_MAN_COUNT', 'DISP_SCAN_MAN_COST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df5 = pd.merge(joined_df1, loc_doc, how ='outer', left_on ='REC_USER', right_on = 'LOC_USER') \n",
    "joined_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df5.columns = ['REC_USER', 'REC_CREATION', 'REC_CREAT_COUNT', 'REC_CREAT_COST', \n",
    "                      'REC_CLASS', 'REC_CLASS_COUNT', 'REC_CLASS_COST', 'LOC_USER', 'LOC_DOC', \n",
    "                      'LOC_DOC_COUNT', 'LOC_DOC_COST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6 = pd.merge(joined_df5, joined_df4, how ='outer', left_on = 'REC_USER', right_on = 'DISP_USER') \n",
    "joined_df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating single column with USER_ID\n",
    "list_userid = []\n",
    "\n",
    "for idx in joined_df6.index:\n",
    "    for col in joined_df6.columns:\n",
    "        if col == \"REC_USER\" and pd.notnull(joined_df6[col][idx]):\n",
    "            list_userid.append(joined_df6[col][idx])\n",
    "        elif col == \"LOC_USER\" and pd.isnull(joined_df6[\"REC_USER\"][idx]) and pd.notnull(joined_df6[col][idx]):\n",
    "            list_userid.append(joined_df6[col][idx])\n",
    "        elif col == \"DISP_USER\" and pd.isnull(joined_df6[\"REC_USER\"][idx]) and pd.isnull(joined_df6[\"LOC_USER\"][idx]) and pd.notnull(joined_df6[col][idx]):\n",
    "            list_userid.append(joined_df6[col][idx])\n",
    "\n",
    "len(list_userid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(0, 'USER_ID', list_userid)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the columns 'REC_USER', 'LOC_USER', 'DISP_USER'\n",
    "joined_df6 = joined_df6.drop(['REC_USER', 'LOC_USER', 'DISP_USER'], 1)\n",
    "joined_df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list for Receiving Module error flag \n",
    "list_module_rec =[]\n",
    "\n",
    "for idx in joined_df6.index:\n",
    "    if (joined_df6['REC_CREATION'][idx] == '1') | (joined_df6['REC_CLASS'][idx] == '1'):\n",
    "        list_module_rec.append(1)\n",
    "    elif pd.isnull(joined_df6['REC_CREATION'][idx]) and pd.isnull(joined_df6['REC_CLASS'][idx]):\n",
    "        list_module_rec.append(0)\n",
    "\n",
    "print(list_module_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list for Location Module error flag \n",
    "list_module_loc =[]\n",
    "\n",
    "for idx in joined_df6.index:\n",
    "    if joined_df6['LOC_DOC'][idx] == '1':\n",
    "        list_module_loc.append(1)\n",
    "    elif pd.isnull(joined_df6['LOC_DOC'][idx]):\n",
    "        list_module_loc.append(0)\n",
    "        \n",
    "print(list_module_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list for Disposal Module error flag \n",
    "list_module_disp =[]\n",
    "\n",
    "for idx in joined_df6.index:\n",
    "    if (joined_df6['DISP_RET_YEAR'][idx] == '1') | (joined_df6['DISP_SCAN_MIS'][idx] == '1') | (joined_df6['DISP_SCAN_MAN'][idx] == '0.5'):\n",
    "        list_module_disp.append(1)\n",
    "    elif pd.isnull(joined_df6['DISP_RET_YEAR'][idx]) and pd.isnull(joined_df6['DISP_SCAN_MIS'][idx]) and pd.isnull(joined_df6['DISP_SCAN_MAN'][idx]):\n",
    "        list_module_disp.append(0)\n",
    "\n",
    "print(list_module_disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column for error in more than 1 module \n",
    "list_module = [x + y + z for x, y, z in zip(list_module_rec, list_module_loc, list_module_disp)]\n",
    "print(list_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_more1_module = []\n",
    "\n",
    "for each in list_module:\n",
    "    if each > 1:\n",
    "        list_more1_module.append(1)\n",
    "    else:\n",
    "        list_more1_module.append(0)\n",
    "\n",
    "print(list_more1_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(22, 'MORE_1_MODULE', list_more1_module)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the data types of a few columns to numeric\n",
    "joined_df6['REC_CREATION'] = pd.to_numeric(joined_df6['REC_CREATION'], errors='coerce')\n",
    "joined_df6['REC_CLASS'] = pd.to_numeric(joined_df6['REC_CLASS'], errors='coerce')\n",
    "joined_df6['LOC_DOC'] = pd.to_numeric(joined_df6['LOC_DOC'], errors='coerce')\n",
    "joined_df6['DISP_DOC'] = pd.to_numeric(joined_df6['DISP_DOC'], errors='coerce')\n",
    "joined_df6['DISP_RET_YEAR'] = pd.to_numeric(joined_df6['DISP_RET_YEAR'], errors='coerce')\n",
    "joined_df6['DISP_SCAN_MIS'] = pd.to_numeric(joined_df6['DISP_SCAN_MIS'], errors='coerce')\n",
    "joined_df6['DISP_SCAN_MAN'] = pd.to_numeric(joined_df6['DISP_SCAN_MAN'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the counts and total costs \n",
    "\n",
    "list1 = joined_df6['REC_CREAT_COUNT']\n",
    "\n",
    "def normalize_list(list1):\n",
    "    list_norm_creat = minmax_scale(list1)\n",
    "    return list_norm_creat\n",
    "\n",
    "list_norm_creat_count = normalize_list(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = joined_df6['REC_CREAT_COST']\n",
    "list_norm_creat_cost = normalize_list(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list3 = joined_df6['REC_CLASS_COUNT']\n",
    "list_norm_class_count = normalize_list(list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list4 = joined_df6['REC_CLASS_COST']\n",
    "list_norm_class_cost = normalize_list(list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list5 = joined_df6['LOC_DOC_COUNT']\n",
    "list_norm_locdoc_count = normalize_list(list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list6 = joined_df6['LOC_DOC_COST']\n",
    "list_norm_locdoc_cost = normalize_list(list6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list7 = joined_df6['DISP_DOC_COUNT']\n",
    "list_norm_dispdoc_count = normalize_list(list7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list8 = joined_df6['DISP_DOC_COST']\n",
    "list_norm_dispdoc_cost = normalize_list(list8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list9 = joined_df6['DISP_RET_COUNT']\n",
    "list_norm_dispret_count = normalize_list(list9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list10 = joined_df6['DISP_RET_COST']\n",
    "list_norm_dispret_cost = normalize_list(list10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list11 = joined_df6['DISP_SCAN_MIS_COUNT']\n",
    "list_norm_dispscanmis_count = normalize_list(list11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list12 = joined_df6['DISP_SCAN_MIS_COST']\n",
    "list_norm_dispscanmis_cost = normalize_list(list12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list13 = joined_df6['DISP_SCAN_MAN_COUNT']\n",
    "list_norm_dispscanman_count = normalize_list(list13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list14 = joined_df6['DISP_SCAN_MAN_COST']\n",
    "list_norm_dispscanman_cost = normalize_list(list14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(3, 'NORM_CREAT_COUNT', list_norm_creat_count)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(5, 'NORM_CREAT_COST', list_norm_creat_cost)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(8, 'NORM_CLASS_COUNT', list_norm_class_count)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(10, 'NORM_CLASS_COST', list_norm_class_cost)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(13, 'NORM_LOCDOC_COUNT', list_norm_locdoc_count)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(15, 'NORM_LOCDOC_COST', list_norm_locdoc_cost)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(18, 'NORM_DISPDOC_COUNT', list_norm_dispdoc_count)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(20, 'NORM_DISPDOC_COST', list_norm_dispdoc_cost)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(23, 'NORM_DISPRET_COUNT', list_norm_dispret_count)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(25, 'NORM_DISPRET_COST', list_norm_dispret_cost)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(28, 'NORM_DISPSCANMIS_COUNT', list_norm_dispscanmis_count)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(30, 'NORM_DISPSCANMIS_COST', list_norm_dispscanmis_cost)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(33, 'NORM_DISPSCANMAN_COUNT', list_norm_dispscanman_count)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.insert(35, 'NORM_DISPSCANMAN_COST', list_norm_dispscanman_cost)\n",
    "joined_df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given weights\n",
    "creation_count_wt = 1\n",
    "creation_cost_wt = 2\n",
    "class_count_wt = 0.5\n",
    "class_cost_wt = 1\n",
    "loc_doc_count_wt = 1\n",
    "loc_doc_cost_wt = 2\n",
    "disp_doc_count_wt = 1\n",
    "disp_doc_cost_wt = 2\n",
    "disp_ret_count_wt =  0.5\n",
    "disp_ret_cost_wt = 1\n",
    "disp_scanmis_count_wt = 1 \n",
    "disp_scanmis_cost_wt = 2\n",
    "disp_scanman_count_wt = 1\n",
    "disp_scanman_cost_wt = 2\n",
    "more_1_module_wt = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying transaction count weights (default = 1)\n",
    "creat_ct_wt = 1\n",
    "class_ct_wt = 1\n",
    "locdoc_ct_wt = 1\n",
    "dispdoc_ct_wt = 1\n",
    "dispret_ct_wt = 1\n",
    "dispscanmis_ct_wt = 1\n",
    "dispscanman_ct_wt = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df6.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe by replacing NaN with '0'\n",
    "final_df = joined_df6.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column with 'WEIGHTED_SCORE'\n",
    "final_df = final_df.assign(\n",
    "    WEIGHTED_SCORE = final_df.REC_CREATION*final_df.NORM_CREAT_COUNT*creation_count_wt*creat_ct_wt + \n",
    "                     final_df.REC_CREATION*final_df.NORM_CREAT_COST*creation_cost_wt*creat_ct_wt + \n",
    "                     final_df.REC_CLASS*final_df.NORM_CLASS_COUNT*class_count_wt*class_ct_wt +\n",
    "                     final_df.REC_CLASS*final_df.NORM_CLASS_COST*class_cost_wt*class_ct_wt +\n",
    "                     final_df.LOC_DOC*final_df.NORM_LOCDOC_COUNT*loc_doc_count_wt*locdoc_ct_wt +  \n",
    "                     final_df.LOC_DOC*final_df.NORM_LOCDOC_COST*loc_doc_cost_wt*locdoc_ct_wt + \n",
    "                     final_df.DISP_DOC*final_df.NORM_DISPDOC_COUNT*disp_doc_count_wt*dispdoc_ct_wt +\n",
    "                     final_df.DISP_DOC*final_df.NORM_DISPDOC_COST*disp_doc_cost_wt*dispdoc_ct_wt +\n",
    "                     final_df.DISP_RET_YEAR*final_df.NORM_DISPRET_COUNT*disp_ret_count_wt*dispret_ct_wt + \n",
    "                     final_df.DISP_RET_YEAR*final_df.NORM_DISPRET_COST*disp_ret_cost_wt*dispret_ct_wt + \n",
    "                     final_df.DISP_SCAN_MIS*final_df.NORM_DISPSCANMIS_COUNT*disp_scanmis_count_wt*dispscanmis_ct_wt + \n",
    "                     final_df.DISP_SCAN_MIS*final_df.NORM_DISPSCANMIS_COST*disp_scanmis_cost_wt*dispscanmis_ct_wt + \n",
    "                     final_df.DISP_SCAN_MAN*final_df.NORM_DISPSCANMAN_COUNT*disp_scanman_count_wt*dispscanman_ct_wt +\n",
    "                     final_df.DISP_SCAN_MAN*final_df.NORM_DISPSCANMAN_COST*disp_scanman_cost_wt*dispscanman_ct_wt +\n",
    "                     final_df.MORE_1_MODULE*more_1_module_wt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-wise list sorted by weighted score\n",
    "user_list_df = final_df.sort_values(by = 'WEIGHTED_SCORE', ascending = False, ignore_index=True).round(4)\n",
    "user_list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prioritized User-wise list with weighted score\n",
    "pri_user_list = user_list_df[['USER_ID','WEIGHTED_SCORE']]\n",
    "pri_user_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the user list as .csv file to bucket\n",
    "file_name = 'user_list.csv'\n",
    "user_list_df.to_csv(file_name)\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file(file_name, 'daen690-meraki-data', 'user_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the styled image in local bucket\n",
    "pri_user_list_styled = pri_user_list[:20].style.background_gradient()\n",
    "pri_user_list_styled\n",
    "dfi.export(pri_user_list_styled, 'top_users_list.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list as image\n",
    "\n",
    "def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax\n",
    "\n",
    "fig,ax = render_mpl_table(pri_user_list[:20], header_columns=0, col_width=4.0)\n",
    "fig.savefig(\"top_users_list.png\")\n",
    "\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('daen690-meraki-dashboard-sample')\n",
    "bucket.put_object(Body=img_data, ContentType='image/png', Key='top_users_list.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(x='USER_ID', y='WEIGHTED_SCORE', data=pri_user_list[:50], color = 'blue')\n",
    "p.set_xticklabels(p.get_xticklabels(),rotation=90)\n",
    "p.set(xlabel='Users', ylabel='Weighted Score')\n",
    "p.set_title('Top 50 Users with highest Weighted Score')\n",
    "plt.gcf().set_size_inches(13, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(y='USER_ID', x='WEIGHTED_SCORE', data=pri_user_list[:20], color = 'blue')\n",
    "p.set(ylabel='Users', xlabel='Weighted Score')\n",
    "p.set_title('Top 20 Users with highest Weighted Score')\n",
    "plt.gcf().set_size_inches(9, 10)\n",
    "\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('daen690-meraki-dashboard-sample')\n",
    "bucket.put_object(Body=img_data, ContentType='image/png', Key='top_users_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_more_mod = user_list_df[user_list_df.MORE_1_MODULE==1].reset_index()\n",
    "user_more_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mod_list = user_more_mod[['USER_ID','WEIGHTED_SCORE']]\n",
    "user_mod_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the styled image in local bucket\n",
    "user_mod_list_styled = user_mod_list[:20].style.background_gradient()\n",
    "user_mod_list_styled\n",
    "dfi.export(user_mod_list_styled, 'user_module_list.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list as image\n",
    "\n",
    "def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax\n",
    "\n",
    "fig,ax = render_mpl_table(user_mod_list[:20], header_columns=0, col_width=4.0)\n",
    "fig.savefig(\"user_module_list.png\")\n",
    "\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('daen690-meraki-dashboard-sample')\n",
    "bucket.put_object(Body=img_data, ContentType='image/png', Key='user_module_list.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(x='USER_ID', y='WEIGHTED_SCORE', data=user_more_mod, color = 'brown')\n",
    "p.set_xticklabels(p.get_xticklabels(),rotation=90)\n",
    "p.set(xlabel='Users', ylabel='Weighted Score')\n",
    "p.set_title('Users with errors in more than one module')\n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(y='USER_ID', x='WEIGHTED_SCORE', data=user_more_mod[:20], color = 'brown')\n",
    "p.set(xlabel='Weighted Score', ylabel= 'Users')\n",
    "p.set_title('Top 20 Users with Errors in More than One Module')\n",
    "plt.gcf().set_size_inches(9,10)\n",
    "\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('daen690-meraki-dashboard-sample')\n",
    "bucket.put_object(Body=img_data, ContentType='image/png', Key='top_users_module_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for location-wise users\n",
    "loc_1 = loc_1.drop(columns='COUNT')\n",
    "loc_2 = loc_2.drop(columns='COUNT')\n",
    "loc_3 = loc_3.drop(columns='COUNT')\n",
    "loc_4 = loc_4.drop(columns='COUNT')\n",
    "loc_5 = loc_5.drop(columns='COUNT')\n",
    "loc_6 = loc_6.drop(columns='COUNT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_1.columns = ['LOCATION', 'USER_ID']\n",
    "loc_2.columns = ['LOCATION', 'USER_ID']\n",
    "loc_3.columns = ['LOCATION', 'USER_ID']\n",
    "loc_4.columns = ['LOCATION', 'USER_ID']\n",
    "loc_5.columns = ['LOCATION', 'USER_ID']\n",
    "loc_6.columns = ['LOCATION', 'USER_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_user_df = pd.concat([loc_1, loc_2, loc_3, loc_4, loc_5, loc_6]).drop_duplicates().sort_values(\n",
    "                        by = ['LOCATION', 'USER_ID'], ascending = [True, True], ignore_index=True)\n",
    "loc_user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location-wise prioritized user list\n",
    "loc_user_list = pd.merge(loc_user_df, final_df, how ='outer', on = 'USER_ID').sort_values(\n",
    "    by = ['LOCATION', 'WEIGHTED_SCORE'], ascending = [True, False], ignore_index=True).round(4)\n",
    "loc_user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location-wise user list with weighted score\n",
    "pri_locuser_list = loc_user_list[['LOCATION','USER_ID','WEIGHTED_SCORE']]\n",
    "pri_locuser_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the styled image in local bucket\n",
    "pri_locuser_list_styled = pri_locuser_list[:20].style.background_gradient()\n",
    "pri_locuser_list_styled\n",
    "dfi.export(pri_locuser_list_styled, 'loc_user_list.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list as image\n",
    "\n",
    "def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax\n",
    "\n",
    "fig,ax = render_mpl_table(pri_locuser_list[:20], header_columns=0, col_width=4.0)\n",
    "fig.savefig(\"loc_user_list.png\")\n",
    "\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('daen690-meraki-dashboard-sample')\n",
    "bucket.put_object(Body=img_data, ContentType='image/png', Key='loc_user_list.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the location wise user list as .csv file to bucket\n",
    "file_name = 'location_user_list.csv'\n",
    "loc_user_list.to_csv(file_name)\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file(file_name, 'daen690-meraki-data', 'location_user_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location-wise list with total weighted score for location\n",
    "loc_list = loc_user_list.groupby(\"LOCATION\").agg(\n",
    "            AGG_WT_SCORE = (\"WEIGHTED_SCORE\",\"sum\"),\n",
    "            USER_COUNT = (\"WEIGHTED_SCORE\",\"count\")\n",
    ").sort_values(by = 'AGG_WT_SCORE', ascending = False).round(2).reset_index()\n",
    "loc_list['AVG_WT_SCORE'] = loc_list.AGG_WT_SCORE/loc_list.USER_COUNT\n",
    "loc_list.index += 1\n",
    "loc_list.AVG_WT_SCORE = loc_list.AVG_WT_SCORE.round(2)\n",
    "loc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the location wise list as .csv file to bucket\n",
    "file_name = 'location_list.csv'\n",
    "loc_list.to_csv(file_name)\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file(file_name, 'daen690-meraki-data', 'location_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 Locations with highest Aggregate Weighted Score\n",
    "loc_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the styled image in local bucket\n",
    "loc_list_styled = loc_list[:20].style.background_gradient()\n",
    "loc_list_styled\n",
    "dfi.export(loc_list_styled, 'loc_list.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list as image\n",
    "\n",
    "def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax\n",
    "\n",
    "fig,ax = render_mpl_table(loc_list[:20], header_columns=0, col_width=4.0)\n",
    "fig.savefig(\"loc_list.png\")\n",
    "\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('daen690-meraki-dashboard-sample')\n",
    "bucket.put_object(Body=img_data, ContentType='image/png', Key='loc_list.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(x='LOCATION', y='AGG_WT_SCORE', data=loc_list[:50], color = 'navy')\n",
    "p.set_xticklabels(p.get_xticklabels(),rotation=90)\n",
    "p.set(xlabel='Location', ylabel='Aggregate Weighted Score')\n",
    "p.set_title('Top 50 Locations with Highest Aggregate Weighted Score')\n",
    "plt.gcf().set_size_inches(14, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(y='LOCATION', x='AGG_WT_SCORE', data=loc_list[:20:], color = 'navy')\n",
    "p.set(ylabel='Location', xlabel='Aggregate Weighted Score')\n",
    "p.set_title('Top 20 Locations with Highest Aggregate Weighted Score')\n",
    "plt.gcf().set_size_inches(11,11)\n",
    "\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('daen690-meraki-dashboard-sample')\n",
    "bucket.put_object(Body=img_data, ContentType='image/png', Key='top_locations_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eruser_ct_rec = sum(list_module_rec)\n",
    "eruser_ct_rec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eruser_ct_loc = sum(list_module_loc)\n",
    "eruser_ct_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eruser_ct_disp = sum(list_module_disp)\n",
    "eruser_ct_disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eruser_ct_tot = user_list_df.USER_ID.nunique()\n",
    "eruser_ct_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ct_rec = df_receiving.OPR_ID.nunique()\n",
    "user_ct_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ct_loc = df_locations.ENTERED_BY.nunique()\n",
    "user_ct_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ct_disp = df_disposals.USER_ID.nunique()\n",
    "user_ct_disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {'MODULE':['Receiving', 'Transfering', 'Disposal'],\n",
    "     'TOTAL_USERS':[user_ct_rec, user_ct_loc, user_ct_disp],\n",
    "     'USERS_W_ERROR':[eruser_ct_rec , eruser_ct_loc, eruser_ct_disp]}\n",
    "df_summary1 = pd.DataFrame(data=d1)\n",
    "df_summary1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list as image\n",
    "\n",
    "def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax\n",
    "\n",
    "fig,ax = render_mpl_table(df_summary1, header_columns=0, col_width=4.0)\n",
    "fig.savefig(\"total_error_user_list.png\")\n",
    "\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('daen690-meraki-dashboard-sample')\n",
    "bucket.put_object(Body=img_data, ContentType='image/png', Key='total_error_user_list.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {'MODULE':['Receiving', 'Transfering', 'Disposal', 'Receiving', 'Transfering', 'Disposal'],\n",
    "     'USERS':['TOTAL', 'TOTAL', 'TOTAL', 'ERROR', 'ERROR', 'ERROR'],\n",
    "     'COUNT':[user_ct_rec, user_ct_loc, user_ct_disp, eruser_ct_rec , eruser_ct_loc, eruser_ct_disp],}\n",
    "df_summary2 = pd.DataFrame(data=d2)\n",
    "df_summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = {'SUMMARY_STATISTIC':['Total Users with Errors', 'Users with Errors >1 Module', \n",
    "                           'Locations with Errors'],\n",
    "     'COUNT':[user_list_df.shape[0], user_more_mod.shape[0], loc_list.shape[0]]}\n",
    "df_summary3 = pd.DataFrame(data=d3)\n",
    "df_summary3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list as image\n",
    "\n",
    "def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax\n",
    "\n",
    "fig,ax = render_mpl_table(df_summary3, header_columns=0, col_width=4.0)\n",
    "fig.savefig(\"summary_table.png\")\n",
    "\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(dashboard_bucket)\n",
    "bucket.put_object(Body=img_data, ContentType='image/png', Key='summary_table.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(x='MODULE', y='COUNT', hue = 'USERS', data=df_summary2)\n",
    "p.set_title('Module-wise Total Users and Users with Errors Count')\n",
    "plt.gcf().set_size_inches(7, 7)\n",
    "\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(dashboard_bucket)\n",
    "bucket.put_object(Body=img_data, ContentType='image/png', Key='total_error_user_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
